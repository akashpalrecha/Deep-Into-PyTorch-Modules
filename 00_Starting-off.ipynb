{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp Starting-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking around\n",
    "\n",
    "> We'll try and get a broad overview of nn.Module in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%config autocompleter.use_jedi=False\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "randn = torch.randn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def print_all(*args, **kwargs):\n",
    "    \"\"\" Prints all parameters on separate lines \"\"\"\n",
    "    for i in args:\n",
    "        print(i)\n",
    "    for key, value in kwargs.items():\n",
    "        print(f\"{key} : {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__call__, __class__, __delattr__, __dict__, __dir__, __doc__, __eq__, __format__, __ge__, __getattr__, __getattribute__, __gt__, __hash__, __init__, __init_subclass__, __le__, __lt__, __module__, __ne__, __new__, __reduce__, __reduce_ex__, __repr__, __setattr__, __setstate__, __sizeof__, __str__, __subclasshook__, __weakref__, _apply, _get_name, _load_from_state_dict, _named_members, _register_load_state_dict_pre_hook, _register_state_dict_hook, _slow_forward, _tracing_name, _version, add_module, apply, buffers, children, cpu, cuda, double, dump_patches, eval, extra_repr, float, forward, half, load_state_dict, modules, named_buffers, named_children, named_modules, named_parameters, parameters, register_backward_hook, register_buffer, register_forward_hook, register_forward_pre_hook, register_parameter, share_memory, state_dict, to, train, type, zero_grad\n"
     ]
    }
   ],
   "source": [
    "print(', '.join(dir(nn.Module)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try cover as many important parts of the `nn.Module` class as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### nn.Module's documentaions says:\n",
    "\n",
    "---\n",
    "CLASS `torch.nn.Module`:\n",
    "\n",
    "*Base class for all neural network modules.*\n",
    "\n",
    "*Your models should also subclass this class.*\n",
    "\n",
    "*Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:*\n",
    "```python\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "        self.conv2 = nn.Conv2d(20, 20, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return F.relu(self.conv2(x))\n",
    "```\n",
    "*Submodules assigned in this way will be registered, and will have their parameters converted too when you call `to()`, etc.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to note:\n",
    "1. Modules can have submodules\n",
    "2. Every module is **registered**. We'll explore what this means soon.\n",
    "3. The method `to()`. We will look into this too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a basic CNN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_1(nn.Module):\n",
    "    \"\"\" A basic CNN Model with Adaptive Pooling to allow varying inpup sizes \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Model_1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 4, 3)\n",
    "        self.conv2 = nn.Conv2d(4, 16, 3)\n",
    "        self.conv3 = nn.Conv2d(16, 4, 3)\n",
    "        self.pool1 = nn.AdaptiveAvgPool2d(1)\n",
    "        self.lin1  = nn.Linear(4, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool1(x)\n",
    "        x = x.view(x.shape[0], -1) # Flattening X\n",
    "        return self.lin1(x)\n",
    "\n",
    "net = Model_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def test_net(net, x=randn(4, 3, 100, 100)):\n",
    "    \"\"\" A convenience function to test out all the various models that we will be creating \"\"\"\n",
    "    out = net(x)\n",
    "    print_all(out.shape, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n",
      "tensor([[0.2610, 0.0375],\n",
      "        [0.2608, 0.0372],\n",
      "        [0.2610, 0.0374],\n",
      "        [0.2610, 0.0372]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "test_net(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Our Goals Today:\n",
    "1. We will try and create the above network in as many ways as possible using the functions available to us and we will go on slight tangents along the way as we find more interesting things.\n",
    "2. We will look into the source code for nn.Module to gain more insight into it's internals.\n",
    "3. We will also see how some of the convenience functions provided in the module can help us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal 1: Recreating `Model_1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the possible ways:\n",
    "```python\n",
    "1. nn.ModuleList\n",
    "2. nn.ModuleDict\n",
    "3. add_module\n",
    "4. nn.ParameterList\n",
    "5. nn.ParameterDict\n",
    "6. register_parameter\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `nn.Sequential` : The easiest alterenative\n",
    "> Not necessarily the most convenient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LambdaModule(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super(LambdaModule, self).__init__()\n",
    "        self.func = func\n",
    "    def forward(self, x):\n",
    "        return self.func(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Flatten = LambdaModule(lambda x: x.view(x.shape[0], -1))\n",
    "Flatten(randn(3,4,4)).shape # Testing Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Conv2d(3, 4, 3),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Conv2d(4, 16, 3),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Conv2d(16, 4, 3),\n",
    "                      nn.ReLU(),\n",
    "                      nn.AdaptiveAvgPool2d(1),\n",
    "                      Flatten,\n",
    "                      nn.Linear(4, 2)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n",
      "tensor([[0.4961, 0.4088],\n",
      "        [0.4950, 0.4097],\n",
      "        [0.4956, 0.4092],\n",
      "        [0.4945, 0.4100]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "test_net(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need this a lot going ahead\n",
    "def get_module_list():\n",
    "    modules = [nn.Conv2d(3, 4, 3),\n",
    "               nn.ReLU(),\n",
    "               nn.Conv2d(4, 16, 3),\n",
    "               nn.ReLU(),\n",
    "               nn.Conv2d(16, 4, 3),\n",
    "               nn.ReLU(),\n",
    "               nn.AdaptiveAvgPool2d(1),\n",
    "               Flatten,\n",
    "               nn.Linear(4, 2)]\n",
    "    return modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `nn.ModuleList`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Provide description here of how it works*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, layers:list):\n",
    "        super(Model, self).__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ModuleList can act as an iterable, or be indexed using ints\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Model(get_module_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n",
      "tensor([[ 0.1547, -0.3233],\n",
      "        [ 0.1546, -0.3233],\n",
      "        [ 0.1551, -0.3236],\n",
      "        [ 0.1553, -0.3235]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "test_net(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
